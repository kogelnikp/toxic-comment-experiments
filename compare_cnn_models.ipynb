{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/philipp/miniconda3/envs/tensorflow/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import utils\n",
    "\n",
    "from keras.models import Model, Input\n",
    "from keras.layers import Dense, Conv1D, BatchNormalization, GlobalMaxPooling1D, Dropout, Embedding\n",
    "\n",
    "from utils.preprocessing_utils import tokenize_sentences, convert_tokens_to_padded_sequence\n",
    "from utils.dataset_utils import load_data_from_csv\n",
    "from utils.embedding_utils import load_word2vec_embeddings, create_initial_embedding_matrix\n",
    "from utils.training_utils import train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils.keras_utils' from '/home/philipp/work/gitprojects/toxic-comment-experiments/utils/keras_utils.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import utils\n",
    "importlib.reload(utils.embedding_utils)\n",
    "importlib.reload(utils.dataset_utils)\n",
    "importlib.reload(utils.preprocessing_utils)\n",
    "importlib.reload(utils.training_utils)\n",
    "importlib.reload(utils.keras_utils)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Global parameters which hold for all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random_seed = 2018\n",
    "classes = ['toxic','severe_toxic','obscene','threat','insult','identity_hate']\n",
    "features = 'comment_text'\n",
    "np.random.seed(random_seed)\n",
    "path_train_data = 'data/kaggle/train.csv'\n",
    "path_test_data = 'data/kaggle/test_complete.csv'\n",
    "path_tokenizer = 'data/models/word_tokenizer.pickle'\n",
    "\n",
    "embedding_length = 300\n",
    "path_embeddings = 'data/embeddings/GoogleNews-vectors-negative300.bin.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load train and test data and pretrained word2vec embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = load_data_from_csv(path_train_data, features, classes)\n",
    "X_test, Y_test = load_data_from_csv(path_test_data, features, classes)\n",
    "\n",
    "emb_idx, emb_mean, emb_std = load_word2vec_embeddings(path_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing and tokenizatin of train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tok = tokenize_sentences(X_train)\n",
    "del X_train\n",
    "X_test_tok = tokenize_sentences(X_test)\n",
    "del X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create initial embedding matrix for neural network and word -> idx mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 326175\n",
      "Number of tokens found in pretrained embeddings: 74211\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix, word_embedding_mapping = create_initial_embedding_matrix(X_train_tok, X_test_tok, emb_idx, emb_mean, emb_std, embedding_length, debug=True)\n",
    "del emb_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform comments in train and test data to padded matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_train = X_train_tok.apply(lambda x: len(x)).max()\n",
    "max_len_test = X_test_tok.apply(lambda x: len(x)).max()\n",
    "# limit length to 2000, otherwise we get a MemoryError\n",
    "max_comment_length = 2000 #max(max_len_train, 2000)\n",
    "X_train_input = convert_tokens_to_padded_sequence(X_train_tok, word_embedding_mapping, max_comment_length)\n",
    "del X_train_tok\n",
    "X_test_input = convert_tokens_to_padded_sequence(X_test_tok, word_embedding_mapping, max_comment_length)\n",
    "del X_test_tok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singlelayer CNN with a single window size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simple CNN consists of an embedding layer, a single convolution layer with a fixed window size and a fully connected hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m1_kernel_size = 3\n",
    "m1_hidden_dim = 256\n",
    "m1_num_filters = 150\n",
    "m1_dropout = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 2000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 2000, 300)         97852800  \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 2000, 150)         135150    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 2000, 150)         600       \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               38656     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 98,028,748\n",
      "Trainable params: 98,028,448\n",
      "Non-trainable params: 300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "m1_input = Input((max_comment_length,))\n",
    "m1_word_emb = Embedding(input_dim=len(embedding_matrix), output_dim=embedding_length, input_length=max_comment_length, weights=[embedding_matrix])(m1_input)\n",
    "\n",
    "m1_conv1 = Conv1D(kernel_size=m1_kernel_size, filters=m1_num_filters, padding='same')(m1_word_emb)\n",
    "m1_conv1 = BatchNormalization()(m1_conv1)\n",
    "m1_conv1 = GlobalMaxPooling1D()(m1_conv1)\n",
    "\n",
    "m1_fc2 = Dense(m1_hidden_dim, activation='relu')(m1_conv1)\n",
    "m1_dropout2 = Dropout(m1_dropout)(m1_fc2)\n",
    "m1_output = Dense(len(classes), activation='sigmoid')(m1_dropout2)\n",
    "\n",
    "m1_model = Model(inputs=[m1_input], outputs=[m1_output])\n",
    "m1_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 159571 samples, validate on 63978 samples\n",
      "Epoch 1/4\n",
      "159552/159571 [============================>.] - ETA: 0s - loss: 0.0708\n",
      " train: ROC-AUC - epoch: 0 - score: 0.989640\n",
      "\n",
      " val: ROC-AUC - epoch: 0 - score: 0.975268\n",
      "159571/159571 [==============================] - 457s 3ms/step - loss: 0.0708 - val_loss: 0.0683\n",
      "Epoch 2/4\n",
      "159552/159571 [============================>.] - ETA: 0s - loss: 0.0415\n",
      " train: ROC-AUC - epoch: 1 - score: 0.994992\n",
      "\n",
      " val: ROC-AUC - epoch: 1 - score: 0.978218\n",
      "159571/159571 [==============================] - 458s 3ms/step - loss: 0.0415 - val_loss: 0.0682\n",
      "Epoch 3/4\n",
      "159552/159571 [============================>.] - ETA: 0s - loss: 0.0340\n",
      " train: ROC-AUC - epoch: 2 - score: 0.996159\n",
      "\n",
      " val: ROC-AUC - epoch: 2 - score: 0.976215\n",
      "159571/159571 [==============================] - 458s 3ms/step - loss: 0.0340 - val_loss: 0.0693\n",
      "Epoch 4/4\n",
      "159552/159571 [============================>.] - ETA: 0s - loss: 0.0286\n",
      " train: ROC-AUC - epoch: 3 - score: 0.997456\n",
      "\n",
      " val: ROC-AUC - epoch: 3 - score: 0.976341\n",
      "159571/159571 [==============================] - 459s 3ms/step - loss: 0.0286 - val_loss: 0.0843\n",
      "63978/63978 [==============================] - 20s 306us/step\n"
     ]
    }
   ],
   "source": [
    "m1_model, predictions = train_model(m1_model, X_train_input, Y_train, (X_test_input, Y_test), \\\n",
    "                                    4, 64, 'adam', 'binary_crossentropy', [], random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
